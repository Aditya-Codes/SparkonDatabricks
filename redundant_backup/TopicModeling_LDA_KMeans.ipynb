{"cells":[{"cell_type":"code","source":["# Computational and Visualisation Packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string, re\nfrom collections import OrderedDict\nfrom wordcloud import WordCloud\nfrom itertools import chain, groupby\n\n# Pyspark Packages\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import CountVectorizer, NGram\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml import Pipeline\n\n# NLTK Packages\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords \nfrom nltk.stem.wordnet import WordNetLemmatizer"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n</div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["## Helper, Utility and Wrapper Modules to assist in the later part of the notebook"],"metadata":{}},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\ndef lemmatize(token):\n  \"\"\" Function to return lemmatized token\"\"\"\n  return lemmatizer.lemmatize(token)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["punctuation = set(string.punctuation) \ndefault_stopwords = stopwords.words('english')\ncustom_stopwords = ['charliecuntskies ', 'bts', 'imisscath ', 'imisscath', 'mcflyforgermany', 'charliecuntskies', 'delongeday', 'rlly', 'barakatday', 'blah']\nstopwords_f = set(default_stopwords + custom_stopwords)\npattern_repl_list = [\".\", \",\", \"(\", \")\", \"!\", \"@\", \"#\"]\n\nreplacement_patterns = [\n(r\"won't\", \"will not\"),\n(r\"haven't\", \"have not\"),\n(r\"can't\", \"cannot\"),\n(r\"i'm\", \"i am\"),\n(r\"ain't\", \"am not\"),\n(r\"(\\w+)'ll\", \"$1 will\"),\n(r\"(\\w+)n't\", \"$1 not\"),\n(r\"(\\w+)'ve\", \"$1 have\"),\n(r\"(\\w+)'s\", \"$1 is\"),\n(r\"(\\w+)'re\", \"$1 are\"),\n(r\"(\\w+)'d\", \"$1 would\"),\n]\n\ndef preprocessed_chats (text):\n  \"\"\" Function to extract processed column over Spark Dataframe \"\"\"\n  text_token = text.lower().split()\n  \n  text_token_filtered = []\n  for token in text_token:\n    for pattern in pattern_repl_list: token = token.replace(pattern, '') # Limit patterns from pattern_repl_list\n    for pattern in replacement_patterns: token = token.replace(pattern[0], pattern[1]) # Limit patterns from replacement_patterns list\n    text_token_filtered.append(token)\n  \n  text_token_filtered_pre = [word for word in text_token_filtered if len(word) > 4]\n  text_sw_removed = [word for word in text_token_filtered if word not in stopwords_f]\n  text_punc_removed = [word for word in text_sw_removed if word not in punctuation]\n  text_lemmatized = [lemmatize(word) for word in text_punc_removed]\n  return text_lemmatized\n\npreprocessing_udf = udf(preprocessed_chats, ArrayType(StringType()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["def plotter(x_vals, y_vals, xlabel, ylabel, title, color=None):\n  \"\"\" Custom Pyplot Plotter\"\"\"\n  if color is None:\n    color='red'\n  fig, ax = plt.subplots()\n  ax.plot(x_vals, y_vals, color=color)\n  ax.set_title(title)\n  ax.set_xlabel(xlabel)\n  ax.set_ylabel(ylabel)\n  display (fig)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["def expanded_frame(summary_df):\n  \"\"\" Function to return words and termWeights each for a particular cluster\"\"\"\n  df_list = []\n  for index, row in summary_df.toPandas().iterrows():\n    words_list = row['words']\n    words_weight = row['termWeights']\n\n    for itr in range(len(words_list)):\n      expanded_dict = OrderedDict()\n      expanded_dict['cluster'] = row['cluster']\n      expanded_dict['words'] = words_list[itr]\n      expanded_dict['termWeights'] = words_weight[itr]\n      df_list.append(expanded_dict)\n\n  new_df = spark.createDataFrame(df_list)\n  return (new_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def return_cluster_summary(lda_model):\n  \"\"\" Function to retrieve formatted cluster summary for analysis\"\"\"\n  cluster_summary_df_list = []\n  final_df_column_list = ['cluster', 'words', 'termWeights']\n  \n  vocab = lda_model.stages[0].vocabulary \n  topics = lda_model.stages[1].describeTopics()\n  topics_words = topics.rdd.map(lambda row: row['termIndices']) .map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n  \n  for idx, topic in enumerate(topics_words):\n    cluster_summary_dict = OrderedDict()\n    cluster_summary_dict['cluster'] = idx + 1\n    cluster_summary_dict['words'] = [word for word in topic]\n    cluster_summary_df_list.append(cluster_summary_dict)\n\n  summary_pre_df = spark.createDataFrame(cluster_summary_df_list)\n  summary_df = summary_pre_df.join(topics, summary_pre_df.cluster == topics.topic)[final_df_column_list]\n  #new_summary_df = expanded_frame(summary_df)[final_df_column_list]\n  return summary_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["def get_optimal_topics(topic_range, maxIteration, input_df):\n  \"\"\" Function to compute optimal topics within a custom input range \"\"\"\n  ll_vals, lp_vals, cluster_no = [], [], []\n  cv = CountVectorizer(inputCol='tokenized_sentiments', outputCol='features')\n  lda = LDA ()\n  lda_pipeline = Pipeline(stages=[cv, lda])\n  \n  for topic_no in topic_range:\n    paramap = {lda.k: topic_no, lda.maxIter:maxIteration, cv.vocabSize:100000, cv.minDF:5.0}\n    lda_i_model = lda_pipeline.fit (input_df, paramap)\n    lda_i_results = lda_i_model.transform (input_df)\n    \n    ll_vals.append(lda_i_model.stages[1].logLikelihood(lda_i_results))\n    lp_vals.append(lda_i_model.stages[1].logPerplexity(lda_i_results))\n    cluster_no.append(topic_no)\n  \n  return ll_vals, lp_vals, cluster_no"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["def concat(type):\n  \"\"\" Function to concat multiple Array columns in a dataframe into a single column\"\"\"\n  def concat_(*args):\n      return list(chain(*args))\n  return udf(concat_, ArrayType(type))\n\n#PySpark UDF\nconcat_string_arrays = concat(StringType())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["def get_top_ngrams(input_df, cap):\n  \"\"\" Function to return top ngrams grouped by cluster\"\"\"\n  top_ngrams_perCluster = {}\n  \n  for itr in range(0, 12, 1):\n    input_df_sub = input_df.filter(\"cluster_predictions == %d\" % itr)\n    \n    voc_ngram = list(chain(*input_df_sub[['tokenized_sentiments']].toPandas()['tokenized_sentiments'].tolist()))\n    voc_ngram_freq = {key:len(list(group)) for key, group in groupby(voc_ngram)}\n    voc_ngram_freq_sorted = OrderedDict(sorted(voc_ngram_freq.items(), key=lambda it: it[1], reverse=True))\n    top_ngrams_i =  ' '.join(list(set([key.encode('utf-8') for key,value in voc_ngram_freq_sorted.items()[:3000]]))[:cap])\n    \n    top_ngrams_perCluster[itr] = top_ngrams_i\n  \n  return top_ngrams_perCluster"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["%sh \n##  /FileStore/tables/twitter_sentiment_analysis2-9b5de.zip\n#cd /dbfs/FileStore/tables/\n##unzip twitter_sentiment_analysis2-9b5de.zip\n#mkdir -p /dbfs/mnt/lda/\n#cp train.csv /dbfs/mnt/lda/twitter_sentiment.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["twitter_sentiments = spark.read.csv('/mnt/lda/twitter_sentiment.csv', header=True) # Only taking a limited set\ntwitter_sentiments = twitter_sentiments[['SentimentText']]\ntwitter_sentiments = twitter_sentiments.withColumn(\"tokenized_sentiments\", preprocessing_udf(\"SentimentText\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["twitter_sentiments_cv = CountVectorizer(inputCol='tokenized_sentiments', outputCol='features') # Creating featues column\ntwitter_sentiments_lda = LDA () # Training a LDA Model\nLda_Pipeline = Pipeline(stages=[twitter_sentiments_cv, twitter_sentiments_lda])\n\n# Parameterized values\nparamap = {twitter_sentiments_lda.k: 9, twitter_sentiments_lda.maxIter:50, twitter_sentiments_cv.vocabSize:100000, twitter_sentiments_cv.minDF:5.0}\n\nLdaModel = Lda_Pipeline.fit(twitter_sentiments, paramap)\nLdaResults = LdaModel.transform(twitter_sentiments)\n\ncluster_summary_df = return_cluster_summary(LdaModel)\ndisplay (cluster_summary_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cluster</th><th>words</th><th>termWeights</th></tr></thead><tbody><tr><td>1</td><td>List(thank, wish, could, like, got, girl, , love, feel, know)</td><td>List(0.019832085774291935, 0.017667615192240544, 0.01278841902211623, 0.011497980713773105, 0.008808296122910276, 0.007938521356095735, 0.007317337164365658, 0.007141093835725717, 0.0063817789091509735, 0.005941255764071033)</td></tr><tr><td>2</td><td>List(said, thought, yeah, thing, exactly, mean, hate, myweakness, follow, second)</td><td>List(0.03355007465199728, 0.012330520052984035, 0.012009115449253394, 0.010138482418648, 0.010026986684539243, 0.009850389161256641, 0.009019471426372879, 0.00884479560446169, 0.00802454907682287, 0.007851805946270206)</td></tr><tr><td>3</td><td>List(love, wanna, happy, &amp;, u, get, great, lol, day, still)</td><td>List(0.01694661951468619, 0.014224388424454037, 0.012726501889125318, 0.012544694003537237, 0.012131699648805765, 0.01113067164982209, 0.010920503419947622, 0.010556856875636546, 0.008662035576269598, 0.008464833324140148)</td></tr><tr><td>4</td><td>List(know, go, cannot, new, want, like, much, get, that's, lol)</td><td>List(0.017885742334392003, 0.016150452459641116, 0.016025094114560663, 0.01256018472245425, 0.009629492534559745, 0.009618481037404585, 0.009583313167791324, 0.009235953388504622, 0.007909604283676142, 0.007491956161727438)</td></tr><tr><td>5</td><td>List(, good, i am, u, like, get, lol, time, thanks, day)</td><td>List(0.019199325260617323, 0.01502195282466297, 0.009995819493855867, 0.007161779038940221, 0.006584564544502578, 0.0053619964978811555, 0.005168854911937015, 0.005105750354268841, 0.004981154877219053, 0.004862563332871768)</td></tr><tr><td>6</td><td>List(oh, haha, suck, check, two, okay, love, next, talking, away)</td><td>List(0.03831686454509927, 0.01060752592295178, 0.010346307644073011, 0.009513023896561132, 0.009340676232261627, 0.009318685573472941, 0.00887796287217453, 0.0065372890494664005, 0.005621639767919185, 0.005521682373443904)</td></tr><tr><td>7</td><td>List(i am, think, come, sorry, working, u, phone, true, , da)</td><td>List(0.015580323108866735, 0.015370044738176623, 0.01534565738529194, 0.012133787055996487, 0.012111761866006493, 0.011917484139487605, 0.011821265005306648, 0.011501689157690505, 0.008620704450890233, 0.008460183144512958)</td></tr><tr><td>8</td><td>List(everyone, hey, follower, get, pay, add, using, day, train, link)</td><td>List(0.02875325552308516, 0.027086625903748704, 0.022273227169057587, 0.0162124964050247, 0.008924499094948751, 0.00805277279848437, 0.007297107843811206, 0.007245761787022959, 0.005829878821669545, 0.004821426518368476)</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"code","source":["fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8)) = plt.subplots(4, 2)\nwordcloud = WordCloud(background_color='white')\n\nt1.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 1')[['words']].collect()[0].words).encode('utf-8')))\nt2.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 2')[['words']].collect()[0].words).encode('utf-8')))\nt3.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 3')[['words']].collect()[0].words).encode('utf-8')))\nt4.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 4')[['words']].collect()[0].words).encode('utf-8')))\nt5.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 5')[['words']].collect()[0].words).encode('utf-8')))\nt6.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 6')[['words']].collect()[0].words).encode('utf-8')))\nt7.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 7')[['words']].collect()[0].words).encode('utf-8')))\nt8.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 8')[['words']].collect()[0].words).encode('utf-8')))\n\nt1.set_title('Cluster 1')\nt2.set_title('Cluster 2')\nt3.set_title('Cluster 3')\nt4.set_title('Cluster 4')\nt5.set_title('Cluster 5')\nt6.set_title('Cluster 6')\nt7.set_title('Cluster 7')\nt8.set_title('Cluster 8')\n\nt1.figure.set_size_inches(20, 20)\nt2.figure.set_size_inches(20, 20)\nt3.figure.set_size_inches(20, 20)\nt4.figure.set_size_inches(20, 20)\nt5.figure.set_size_inches(20, 20)\nt6.figure.set_size_inches(20, 20)\nt7.figure.set_size_inches(20, 20)\nt8.figure.set_size_inches(20, 20)\n\nt1.set_axis_off()\nt2.set_axis_off()\nt3.set_axis_off()\nt4.set_axis_off()\nt5.set_axis_off()\nt6.set_axis_off()\nt7.set_axis_off()\nt8.set_axis_off()\ndisplay (fig)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-2717300798909995&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      2</span> wordcloud <span class=\"ansiyellow\">=</span> WordCloud<span class=\"ansiyellow\">(</span>background_color<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;white&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span>t1<span class=\"ansiyellow\">.</span>imshow<span class=\"ansiyellow\">(</span>wordcloud<span class=\"ansiyellow\">.</span>generate<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos; &apos;</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>cluster_summary_df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;cluster == 1&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;words&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>encode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;utf-8&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> t2<span class=\"ansiyellow\">.</span>imshow<span class=\"ansiyellow\">(</span>wordcloud<span class=\"ansiyellow\">.</span>generate<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos; &apos;</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>cluster_summary_df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;cluster == 2&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;words&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>encode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;utf-8&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      6</span> t3<span class=\"ansiyellow\">.</span>imshow<span class=\"ansiyellow\">(</span>wordcloud<span class=\"ansiyellow\">.</span>generate<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos; &apos;</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>cluster_summary_df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;cluster == 3&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;words&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>encode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;utf-8&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/wordcloud/wordcloud.py</span> in <span class=\"ansicyan\">generate</span><span class=\"ansiblue\">(self, text)</span>\n<span class=\"ansigreen\">    603</span>         self<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    604</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 605</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>generate_from_text<span class=\"ansiyellow\">(</span>text<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    606</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    607</span>     <span class=\"ansigreen\">def</span> _check_generated<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/wordcloud/wordcloud.py</span> in <span class=\"ansicyan\">generate_from_text</span><span class=\"ansiblue\">(self, text)</span>\n<span class=\"ansigreen\">    584</span>         self<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    585</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 586</span><span class=\"ansiyellow\">         </span>words <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>process_text<span class=\"ansiyellow\">(</span>text<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    587</span>         self<span class=\"ansiyellow\">.</span>generate_from_frequencies<span class=\"ansiyellow\">(</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    588</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/wordcloud/wordcloud.py</span> in <span class=\"ansicyan\">process_text</span><span class=\"ansiblue\">(self, text)</span>\n<span class=\"ansigreen\">    551</span>         regexp <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>regexp <span class=\"ansigreen\">if</span> self<span class=\"ansiyellow\">.</span>regexp <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">None</span> <span class=\"ansigreen\">else</span> <span class=\"ansiblue\">r&quot;\\w[\\w&apos;]+&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    552</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 553</span><span class=\"ansiyellow\">         </span>words <span class=\"ansiyellow\">=</span> re<span class=\"ansiyellow\">.</span>findall<span class=\"ansiyellow\">(</span>regexp<span class=\"ansiyellow\">,</span> text<span class=\"ansiyellow\">,</span> flags<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    554</span>         <span class=\"ansired\"># remove stopwords</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    555</span>         words <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>word <span class=\"ansigreen\">for</span> word <span class=\"ansigreen\">in</span> words <span class=\"ansigreen\">if</span> word<span class=\"ansiyellow\">.</span>lower<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> stopwords<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/re.py</span> in <span class=\"ansicyan\">findall</span><span class=\"ansiblue\">(pattern, string, flags)</span>\n<span class=\"ansigreen\">    211</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    212</span>     Empty matches are included in the result.&quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 213</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> _compile<span class=\"ansiyellow\">(</span>pattern<span class=\"ansiyellow\">,</span> flags<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>findall<span class=\"ansiyellow\">(</span>string<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    214</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    215</span> <span class=\"ansigreen\">def</span> finditer<span class=\"ansiyellow\">(</span>pattern<span class=\"ansiyellow\">,</span> string<span class=\"ansiyellow\">,</span> flags<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: cannot use a string pattern on a bytes-like object</div>"]}}],"execution_count":14},{"cell_type":"code","source":["topic_range = range(6, 15, 2)\nll_vals, lp_vals, cluster_no = get_optimal_topics(topic_range, 50, twitter_sentiments.sample(False, 0.4))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["plotter(cluster_no, ll_vals, xlabel='Clusters', ylabel='LogLikelihood Scores', title='Plot of LogLikelihood against number of clusters', color='green')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["plotter(cluster_no, lp_vals, xlabel='Clusters', ylabel='LogPerplexity Scores', title='Plot of LogPerplexity against number of clusters', color='blue')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Parameterized values\nparamap = {twitter_sentiments_lda.k: 13, twitter_sentiments_lda.maxIter:50, twitter_sentiments_cv.vocabSize:100000, twitter_sentiments_cv.minDF:6.0}\n\nLdaModel = Lda_Pipeline.fit(twitter_sentiments, paramap)\nLdaResults_Best = LdaModel.transform(twitter_sentiments)\n\ncluster_summary_df_Best = return_cluster_summary(LdaModel)\ndisplay (cluster_summary_df_Best)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8), (t9, t10), (t11, t12)) = plt.subplots(6, 2)\nwordcloud = WordCloud(background_color='white')\n\nt1.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 1')[['words']].collect()[0].words).encode('utf-8')))\nt2.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 2')[['words']].collect()[0].words).encode('utf-8')))\nt3.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 3')[['words']].collect()[0].words).encode('utf-8')))\nt4.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 4')[['words']].collect()[0].words).encode('utf-8')))\nt5.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 5')[['words']].collect()[0].words).encode('utf-8')))\nt6.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 6')[['words']].collect()[0].words).encode('utf-8')))\nt7.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 7')[['words']].collect()[0].words).encode('utf-8')))\nt8.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 8')[['words']].collect()[0].words).encode('utf-8')))\nt9.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 9')[['words']].collect()[0].words).encode('utf-8')))\nt10.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 10')[['words']].collect()[0].words).encode('utf-8')))\nt11.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 11')[['words']].collect()[0].words).encode('utf-8')))\nt12.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 12')[['words']].collect()[0].words).encode('utf-8')))\n\nt1.set_title('Cluster 1')\nt2.set_title('Cluster 2')\nt3.set_title('Cluster 3')\nt4.set_title('Cluster 4')\nt5.set_title('Cluster 5')\nt6.set_title('Cluster 6')\nt7.set_title('Cluster 7')\nt8.set_title('Cluster 8')\nt9.set_title('Cluster 9')\nt10.set_title('Cluster 10')\nt11.set_title('Cluster 11')\nt12.set_title('Cluster 12')\n\nt1.figure.set_size_inches(20, 20)\nt2.figure.set_size_inches(20, 20)\nt3.figure.set_size_inches(20, 20)\nt4.figure.set_size_inches(20, 20)\nt5.figure.set_size_inches(20, 20)\nt6.figure.set_size_inches(20, 20)\nt7.figure.set_size_inches(20, 20)\nt8.figure.set_size_inches(20, 20)\nt9.figure.set_size_inches(20, 20)\nt10.figure.set_size_inches(20, 20)\nt11.figure.set_size_inches(20, 20)\nt12.figure.set_size_inches(20, 20)\n\nt1.set_axis_off()\nt2.set_axis_off()\nt3.set_axis_off()\nt4.set_axis_off()\nt5.set_axis_off()\nt6.set_axis_off()\nt7.set_axis_off()\nt8.set_axis_off()\nt9.set_axis_off()\nt10.set_axis_off()\nt11.set_axis_off()\nt12.set_axis_off()\ndisplay (fig)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["ngram_range = range(2, 5, 1)\nngrams = [NGram(n=ngram, inputCol=\"tokenized_sentiments\", outputCol=\"{0}_grams\".format(ngram)) for ngram in ngram_range]\nngrams_pipeline = Pipeline(stages=ngrams)\nngram_df = ngrams_pipeline.fit(LdaResults).transform(LdaResults)\n\n# Merge Ngrams into a single column\nngram_df = ngram_df.select(col('SentimentText'), col('tokenized_sentiments'), col('features'), col('topicDistribution'), concat_string_arrays(col(\"2_grams\"), col(\"3_grams\"), col(\"4_grams\")).alias('ngrams'))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["voc_ngram = list(chain(*ngram_df[['ngrams']].toPandas()['ngrams'].tolist()))\nvoc_ngram_freq = {key:len(list(group)) for key, group in groupby(voc_ngram)}\nvoc_ngram_freq_sorted = OrderedDict(sorted(voc_ngram_freq.items(), key=lambda it: it[1], reverse=True))\ntop_30_ngrams =  ' '.join(list(set([key.encode('utf-8') for key,value in voc_ngram_freq_sorted.items()[:3000]]))[:30])\n\nfig, ax = plt.subplots()\nwordcloud = WordCloud(background_color='black')\nax.imshow(wordcloud.generate(top_30_ngrams))\nax.set_title('Top 30 NGrams')\nax.figure.set_size_inches(10, 10)\nax.set_axis_off()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml.clustering import BisectingKMeans\n\nkmeans_inst = BisectingKMeans(featuresCol='features', predictionCol='cluster_predictions', maxIter=50, k=12, minDivisibleClusterSize=5.0, seed=42)\nkmeans_mod = kmeans_inst.fit(LdaResults)\nLdaResults_kmeans = kmeans_mod.transform(LdaResults)\ncluster_centers = kmeans_mod.clusterCenters()\n\ndisplay (LdaResults_kmeans, 100)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["costs = []\nclusters = []\n\nfor cluster_i in range(6, 15, 2):\n    kmeans_i = BisectingKMeans(featuresCol='features', predictionCol='cluster_predictions', maxIter=30, k=cluster_i, minDivisibleClusterSize=5.0, seed=43)\n    model_i = kmeans_i.fit(LdaResults_kmeans.sample(False, 0.2, seed=42))\n    \n    # Updating the list\n    costs.append(model_i.computeCost(LdaResults_kmeans))\n    clusters.append(cluster_i)\n\n# plotter(x_vals, y_vals, xlabel, ylabel, title, color=None)\nplotter (clusters, costs, 'Clusters', 'Cost', 'Detecting Optimal Number of clusters using KMeans', color='blue')"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["top30_ngrams_perCluster = get_top_ngrams(LdaResults_kmeans, 30)\nfig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8), (t9, t10), (t11, t12)) = plt.subplots(6, 2)\nwordcloud = WordCloud(background_color='white')\n\nt1.imshow(wordcloud.generate(top30_ngrams_perCluster.get(0)))\nt2.imshow(wordcloud.generate(top30_ngrams_perCluster.get(1)))\nt3.imshow(wordcloud.generate(top30_ngrams_perCluster.get(2)))\nt4.imshow(wordcloud.generate(top30_ngrams_perCluster.get(3)))\nt5.imshow(wordcloud.generate(top30_ngrams_perCluster.get(4)))\nt6.imshow(wordcloud.generate(top30_ngrams_perCluster.get(5)))\nt7.imshow(wordcloud.generate(top30_ngrams_perCluster.get(6)))\nt8.imshow(wordcloud.generate(top30_ngrams_perCluster.get(7)))\nt9.imshow(wordcloud.generate(top30_ngrams_perCluster.get(8)))\nt10.imshow(wordcloud.generate(top30_ngrams_perCluster.get(9)))\nt11.imshow(wordcloud.generate(top30_ngrams_perCluster.get(10)))\nt12.imshow(wordcloud.generate(top30_ngrams_perCluster.get(11)))\n\nt1.set_title('Cluster 1')\nt2.set_title('Cluster 2')\nt3.set_title('Cluster 3')\nt4.set_title('Cluster 4')\nt5.set_title('Cluster 5')\nt6.set_title('Cluster 6')\nt7.set_title('Cluster 7')\nt8.set_title('Cluster 8')\nt9.set_title('Cluster 9')\nt10.set_title('Cluster 10')\nt11.set_title('Cluster 11')\nt12.set_title('Cluster 12')\n\nt1.figure.set_size_inches(20, 20)\nt2.figure.set_size_inches(20, 20)\nt3.figure.set_size_inches(20, 20)\nt4.figure.set_size_inches(20, 20)\nt5.figure.set_size_inches(20, 20)\nt6.figure.set_size_inches(20, 20)\nt7.figure.set_size_inches(20, 20)\nt8.figure.set_size_inches(20, 20)\nt9.figure.set_size_inches(20, 20)\nt10.figure.set_size_inches(20, 20)\nt11.figure.set_size_inches(20, 20)\nt12.figure.set_size_inches(20, 20)\n\nt1.set_axis_off()\nt2.set_axis_off()\nt3.set_axis_off()\nt4.set_axis_off()\nt5.set_axis_off()\nt6.set_axis_off()\nt7.set_axis_off()\nt8.set_axis_off()\nt9.set_axis_off()\nt10.set_axis_off()\nt11.set_axis_off()\nt12.set_axis_off()\ndisplay (fig)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["LdaResults_kmeans.createOrReplaceTempView('LdaResults_kmeans')\nLdaResults_kmeans_summary = spark.sql('''with tbl1 as (select cluster_predictions AS summ_cluster_predictions, count(*) count_total from LdaResults_kmeans group by cluster_predictions) \n                             select tbl1.*, row_number() over (order by count_total desc) as cluster_prediction from tbl1 order by count_total desc''')\nLDA_Results_kmeans_summary_expanded = LdaResults_kmeans_summary.join(LdaResults_kmeans, LdaResults_kmeans_summary.summ_cluster_predictions == LdaResults_kmeans.cluster_predictions, how='inner')[['SentimentText', 'tokenized_sentiments', 'count_total', 'cluster_predictions', 'topicDistribution']]\ndisplay (LDA_Results_kmeans_summary_expanded)\nspark.catalog.dropTempView('LdaResults_kmeans')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display (LDA_Results_kmeans_summary_expanded)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["The dataset we used in this notebook came from a specific user's Facebook feed, so, isn't a very great one to start with. However, both, LDA and Bisecting KMeans implementation demonstrated that 12 topics is an optimal number for topic modeling. \nThe published notebook is available at - \nhttps://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3173713035751393/2689111109152757/2308983777460038/latest.html\n\nNote: Data Source used is those of the web scrapped data publicly available from Twitter API from personal twitter handle, and other twitter handles owned by me."],"metadata":{}}],"metadata":{"name":"TopicModeling_LDA_KMeans","notebookId":2689111109152757},"nbformat":4,"nbformat_minor":0}
