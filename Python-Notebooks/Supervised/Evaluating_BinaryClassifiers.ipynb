{"cells":[{"cell_type":"markdown","source":["Dataset source: https://archive.ics.uci.edu/ml/datasets/Adult <br/>\nPySpark ML Classifier Reference: https://spark.apache.org/docs/2.3.0/ml-classification-regression.html#classification <br/>\n<b> <i> Classifier Models Used: </i> </b> <br/>\n<ul>\n  <li> Logistic Regression </li>\n  <li> Naive Bayes </li>\n  <li> Decision Tree </li>\n  <li> Gradient-boosted Tree </li>\n  <li> Random Forest </li>\n  <li> Multilayer Perceptron </li>\n  <li> One-vs-All (Logistic Regression, Random Forest) </li>\n</ul>\n<hr/>"],"metadata":{}},{"cell_type":"code","source":["# Computational and Visualisation Packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ggplot import *\n\n# Pyspark Packages\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, desc, trim\nfrom pyspark.sql.types import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import NaiveBayes, LogisticRegression, DecisionTreeClassifier, GBTClassifier, RandomForestClassifier, OneVsRest, MultilayerPerceptronClassifier\nfrom pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\nfrom pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["adult_gov_data = spark.read.csv('/databricks-datasets/adult/adult.data')\ncountry_codes = spark.sql('SELECT * FROM country_codes')\n\nadult_gov_data = adult_gov_data.select(col('_c0').cast('double').alias('age'), col('_c1').alias('workclass'), col('_c2').cast('double').alias('fnlwgt'), col('_c3').alias('education'), col('_c4').cast('double').alias('education_num'), col('_c5').alias('marital_status'),col('_c6').alias('occupation'), col('_c7').alias('relationship'), col('_c8').alias('race'), col('_c9').alias('sex'), col('_c10').cast('double').alias('capital_gain'), col('_c11').cast('double').alias('capital_loss'), col('_c12').cast('double').alias('hours_per_week'), col('_c13').alias('native_country'), col('_c14').alias('income'))\nadult_gov_data = adult_gov_data.withColumn('native_country', F.regexp_replace(col('native_country'), '-', ' '))\nadult_gov_data = adult_gov_data.withColumn('native_country', trim(col('native_country')))\n\nexpanded_cols = adult_gov_data.columns + ['alpha_3_code', 'latitude', 'longitude']\nadult_gov_data_expanded = adult_gov_data.join(country_codes, adult_gov_data.native_country == country_codes.country, how='left')[expanded_cols]"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display (adult_gov_data_expanded.sample(False, 0.01), 250)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display (adult_gov_data_expanded.describe())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(adult_gov_data_expanded.groupBy('alpha_3_code').agg(F.sum('hours_per_week').alias('total_hours')))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["col_categorical = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\npipeline_steps = [] \n\nfor column in col_categorical:\n    string_indexed = StringIndexer(inputCol=column, outputCol=column + \"Index\")\n    one_hot_encoded = OneHotEncoderEstimator(inputCols=[string_indexed.getOutputCol()], outputCols=[column + \"classVec\"])\n    pipeline_steps += [string_indexed, one_hot_encoded]\n    \nlabel_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\npipeline_steps += [label_stringIdx]\n\nnumerical_columns = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\nassembler_col = [col + \"classVec\" for col in col_categorical] + numerical_columns\nassemblerModel = VectorAssembler(inputCols=assembler_col, outputCol=\"features\")\npipeline_steps += [assemblerModel]\n\n# Applying the pipeline on the dataset\npipelineInst = Pipeline (stages=pipeline_steps)\npipelineModel = pipelineInst.fit (adult_gov_data_expanded)\nadult_gov_data_processed = pipelineModel.transform (adult_gov_data_expanded).select(adult_gov_data_expanded.columns + ['label', 'features'])\ntrain, test = adult_gov_data_processed.randomSplit([.75, .25], seed=121)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display (train.describe())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display_cols = ['label', 'age', 'occupation', 'probability', 'prediction']"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["lrInst = LogisticRegression(labelCol='label', featuresCol='features', maxIter=50)\nlrModel = lrInst.fit(train)\nlrPredictions = lrModel.transform(test)\n\nlrbceInst = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nprint (\"Model Accuracy = %.15f\" % lrbceInst.evaluate(lrPredictions))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display (lrPredictions[display_cols])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["lrpgInst = (ParamGridBuilder()\n             .addGrid(lrInst.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lrInst.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lrInst.maxIter, [15, 30, 45])\n             .build())\nlrcvInst = CrossValidator(estimator=lrInst, estimatorParamMaps=lrpgInst, evaluator=lrbceInst, numFolds=5) #5 folds\nlrcvModel = lrcvInst.fit(train)\nlrcvPredictions = lrcvModel.transform(test)\nprint ('Best Model Score: %.15f' % lrbceInst.evaluate(lrcvPredictions))\nprint ('Best Model Intercept: %.15f' % lrcvModel.bestModel.intercept)\nnew_frame_lrweights = sqlContext.createDataFrame([(float(w),) for w in lrcvModel.bestModel.coefficients], [\"Feature Weight\"])"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(new_frame_lrweights)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(lrcvPredictions[display_cols])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["nbInst = NaiveBayes(labelCol='label', featuresCol='features', smoothing=2.0, modelType=\"multinomial\")\nnbModel = nbInst.fit(train)\nnbPredictions = nbModel.transform(test)\n\nnbmceInst = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nprint (\"Model Accuracy = %.15f\" % nbmceInst.evaluate(nbPredictions))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display (nbPredictions[display_cols])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["nbpgInst = (ParamGridBuilder()\n             .addGrid(nbInst.smoothing, [1.0, 2.0, 3.0, 4.0, 5.0])\n             .build())\nnbcvInst = CrossValidator(estimator=nbInst, estimatorParamMaps=nbpgInst, evaluator=nbmceInst, numFolds=5) #5 folds\nnbcvModel = nbcvInst.fit(train)\nnbcvBestPredictions = nbcvModel.transform(test)\nprint ('Best Model Score: %.15f' % nbmceInst.evaluate(nbcvBestPredictions))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(nbcvBestPredictions)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["lrPrevBestModel = lrcvModel.bestModel\novalrInst = LogisticRegression(labelCol='label', featuresCol='features', fitIntercept=True, \n                               maxIter=lrPrevBestModel._java_obj.getMaxIter(), \n                               elasticNetParam=lrPrevBestModel._java_obj.getElasticNetParam(), \n                               regParam=lrPrevBestModel._java_obj.getRegParam())\novaInst = OneVsRest(classifier=ovalrInst)\novaModel = ovaInst.fit(train)\novaPredictions = ovaModel.transform(test)\n\novamceInst = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint (\"Model Accuracy = %.15f\" % ovamceInst.evaluate(ovaPredictions))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display (ovaPredictions)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["dtInst = DecisionTreeClassifier (labelCol=\"label\", featuresCol=\"features\", maxDepth=4)\ndtModel = dtInst.fit (train)\ndtPredictions = dtModel.transform (test)\ndtbceInst = BinaryClassificationEvaluator()\nprint ('Model Fit Score: %.15f' % dtbceInst.evaluate(dtPredictions))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(dtPredictions[display_cols])"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["dtpgInst = (ParamGridBuilder()\n             .addGrid(dtInst.maxDepth, [1, 3, 8, 12])\n             .addGrid(dtInst.maxBins, [25, 50, 90])\n             .build())\ndtcvInst = CrossValidator(estimator=dtInst, estimatorParamMaps=dtpgInst, evaluator=dtbceInst, numFolds=5) #5 folds\ndtcvModel = dtcvInst.fit(train)\ndtcvBestPredictions = dtcvModel.transform(test)\nprint ('Best Model Score: %.15f' % dtbceInst.evaluate(dtcvBestPredictions))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(dtcvBestPredictions[display_cols])"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["gbtInst = GBTClassifier (labelCol=\"label\", featuresCol=\"features\", maxIter=20, maxDepth=5)\ngbtModel = gbtInst.fit (train)\ngbtPredictions = gbtModel.transform (test)\ngbtmceInst = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nprint ('Model Fit Score: %.15f' % gbtmceInst.evaluate(gbtPredictions))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(gbtPredictions[display_cols])"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["gbtpgInst = (ParamGridBuilder()\n             .addGrid(gbtInst.maxDepth, [3, 5, 7])\n             .addGrid(gbtInst.maxIter, [25, 40, 50])\n             .build())\ngbtcvInst = CrossValidator(estimator=gbtInst, estimatorParamMaps=gbtpgInst, evaluator=gbtmceInst, numFolds=5) #5 folds\ngbtcvModel = gbtcvInst.fit(train)\ngbtcvBestPredictions = gbtcvModel.transform(test)\nprint ('Best Model Score: %.15f' % gbtmceInst.evaluate(gbtcvBestPredictions))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display (gbtcvBestPredictions)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["rfInst = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\nrfModel = rfInst.fit(train)\nrfPredictions = rfModel.transform(test)\nrfbceInst = BinaryClassificationEvaluator()\nprint ('Model Fit Score: %.15f' % rfbceInst.evaluate(rfPredictions))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display (rfPredictions[display_cols])"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["rfpgInst = (ParamGridBuilder()\n             .addGrid(rfInst.maxDepth, [1, 3, 8])\n             .addGrid(rfInst.maxBins, [25, 50])\n             .addGrid(rfInst.numTrees, [5, 20])\n             .build())\nrfcvInst = CrossValidator(estimator=rfInst, estimatorParamMaps=rfpgInst, evaluator=rfbceInst, numFolds=5)\nrfcvModel = rfcvInst.fit(train)\nrfcvBestPredictions = rfcvModel.transform(test)\nprint ('Best Model Score: %.15f' % rfbceInst.evaluate(rfcvBestPredictions))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display (rfcvBestPredictions)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["rfPrevBestModel = rfcvModel.bestModel\novarfInst = RandomForestClassifier(labelCol='label', featuresCol='features',\n                               maxDepth=rfPrevBestModel._java_obj.getMaxDepth(), \n                               maxBins=rfPrevBestModel._java_obj.getMaxBins(), \n                               numTrees=rfPrevBestModel._java_obj.getNumTrees())\novarfInst = OneVsRest(classifier=ovarfInst)\novarfModel = ovarfInst.fit(train)\novarfPredictions = ovarfModel.transform(test)\n\novamceInst = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint (\"Model Accuracy = %.15f\" % ovamceInst.evaluate(ovarfPredictions))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["display (ovarfPredictions)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["layers = [train.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"], 20, 10, 2]\nmpcInst = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=451)\nmpcModel = mpcInst.fit(train)\nmpcPredictions = mpcModel.transform(test)\nmpcBce = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint (\"Model Accuracy = %.15f\" % mpcBce.evaluate(mpcPredictions))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["display (mpcPredictions)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["mpcpgInst = (ParamGridBuilder()\n             .addGrid(mpcInst.maxIter, [50, 100, 150])\n             .addGrid(mpcInst.blockSize, [128, 256, 512])\n             .build())\nmpccvInst = CrossValidator(estimator=mpcInst, estimatorParamMaps=mpcpgInst, evaluator=mpcBce, numFolds=5)\nmpccvModel = mpccvInst.fit(train)\nmpccvBestPredictions = mpccvModel.transform(test)\nprint (\"Best Model Accuracy Score = %.15f\" % mpcBce.evaluate(mpccvBestPredictions))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display (mpccvBestPredictions)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["selectedModel = rfcvModel.bestModel\nselectedPredictions = selectedModel.transform(adult_gov_data_processed)\nprint (\"Model Fit Score = \", rfbceInst.evaluate(selectedPredictions))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["display(selectedPredictions)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["display(selectedPredictions)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["classifier_comparison_df = spark.createDataFrame(pd.DataFrame({\n  'classifiers':['LR', 'NB', 'DT', 'GBT', 'RF', 'MLP', 'ovA-LR', 'ovA-RF'], \n  'accuracy_scores':[90.1, 78.8, 76.7, 85.5, 89.1, 76.5, 84.8, 84.8]\n}))\ndisplay(classifier_comparison_df)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["We implemented all the classifiers available with the Pyspark ML module, and the RF classifier emerged with the best accuracy for this dataset with a score of 90.6%. We plan to implement XGBoost on the same data in the future notebooks. <br/> \n\nFor the best models against each classifier, the below-mentioned accuracy scores were achieved: <br/>\n\n<ul>\n  <li> Logistic Regression - 90.1% </li>\n  <li> Naive Bayes - 78.8% </li>\n  <li> Decision Tree - 76.7% </li>\n  <li> Gradient-boosted Tree - 85.5% </li>\n  <li> Random Forest - 89.1% </li>\n  <li> Multilayer Perceptron - 76.5%</li>\n  <li> One-vs-All (Logistic Regression - 84.8%, Random Forest - 84.8%) </li>\n</ul>\n\nThe published version of the notebook is available at - <br/> \nhttps://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3173713035751393/675963439015456/2308983777460038/latest.html"],"metadata":{}}],"metadata":{"name":"Evaluating_BinaryClassifiers","notebookId":675963439015456},"nbformat":4,"nbformat_minor":0}
