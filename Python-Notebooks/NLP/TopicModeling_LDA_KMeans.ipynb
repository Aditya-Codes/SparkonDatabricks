{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h1> <u> Dataset Sources </u> </h1>\n",
    "\n",
    "<ul> \n",
    "  <li> <a href=\"\"> Twitter Sentiment Data </a> (Last Updated: Feb 4, 2018) </li>\n",
    "  </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
       "[nltk_data]   Package stopwords is already up-to-date!\n",
       "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
       "[nltk_data]   Package wordnet is already up-to-date!\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Computational and Visualisation Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string, re\n",
    "from collections import OrderedDict\n",
    "from wordcloud import WordCloud\n",
    "from itertools import chain, groupby\n",
    "\n",
    "# Pyspark Packages\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer, NGram\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# NLTK Packages\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper, Utility and Wrapper Modules to assist in the later part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(token):\n",
    "  \"\"\" Function to return lemmatized token\"\"\"\n",
    "  return lemmatizer.lemmatize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "punctuation = set(string.punctuation) \n",
    "default_stopwords = stopwords.words('english')\n",
    "custom_stopwords = ['charliecuntskies ', 'bts', 'imisscath ', 'imisscath', 'mcflyforgermany', 'charliecuntskies', 'delongeday', 'rlly', 'barakatday', 'blah']\n",
    "stopwords_f = set(default_stopwords + custom_stopwords)\n",
    "pattern_repl_list = [\".\", \",\", \"(\", \")\", \"!\", \"@\", \"#\"]\n",
    "\n",
    "replacement_patterns = [\n",
    "(r\"won't\", \"will not\"),\n",
    "(r\"haven't\", \"have not\"),\n",
    "(r\"can't\", \"cannot\"),\n",
    "(r\"i'm\", \"i am\"),\n",
    "(r\"ain't\", \"am not\"),\n",
    "(r\"(\\w+)'ll\", \"$1 will\"),\n",
    "(r\"(\\w+)n't\", \"$1 not\"),\n",
    "(r\"(\\w+)'ve\", \"$1 have\"),\n",
    "(r\"(\\w+)'s\", \"$1 is\"),\n",
    "(r\"(\\w+)'re\", \"$1 are\"),\n",
    "(r\"(\\w+)'d\", \"$1 would\"),\n",
    "]\n",
    "\n",
    "def preprocessed_chats (text):\n",
    "  \"\"\" Function to extract processed column over Spark Dataframe \"\"\"\n",
    "  text_token = text.lower().split()\n",
    "  \n",
    "  text_token_filtered = []\n",
    "  for token in text_token:\n",
    "    for pattern in pattern_repl_list: token = token.replace(pattern, '') # Limit patterns from pattern_repl_list\n",
    "    for pattern in replacement_patterns: token = token.replace(pattern[0], pattern[1]) # Limit patterns from replacement_patterns list\n",
    "    text_token_filtered.append(token)\n",
    "  \n",
    "  text_token_filtered_pre = [word for word in text_token_filtered if len(word) > 4]\n",
    "  text_sw_removed = [word for word in text_token_filtered if word not in stopwords_f]\n",
    "  text_punc_removed = [word for word in text_sw_removed if word not in punctuation]\n",
    "  text_lemmatized = [lemmatize(word) for word in text_punc_removed]\n",
    "  return text_lemmatized\n",
    "\n",
    "preprocessing_udf = udf(preprocessed_chats, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotter(x_vals, y_vals, xlabel, ylabel, title, color=None):\n",
    "  \"\"\" Custom Pyplot Plotter\"\"\"\n",
    "  if color is None:\n",
    "    color='red'\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x_vals, y_vals, color=color)\n",
    "  ax.set_title(title)\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel(ylabel)\n",
    "  display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def expanded_frame(summary_df):\n",
    "  \"\"\" Function to return words and termWeights each for a particular cluster\"\"\"\n",
    "  df_list = []\n",
    "  for index, row in summary_df.toPandas().iterrows():\n",
    "    words_list = row['words']\n",
    "    words_weight = row['termWeights']\n",
    "\n",
    "    for itr in range(len(words_list)):\n",
    "      expanded_dict = OrderedDict()\n",
    "      expanded_dict['cluster'] = row['cluster']\n",
    "      expanded_dict['words'] = words_list[itr]\n",
    "      expanded_dict['termWeights'] = words_weight[itr]\n",
    "      df_list.append(expanded_dict)\n",
    "\n",
    "  new_df = spark.createDataFrame(df_list)\n",
    "  return (new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def return_cluster_summary(lda_model):\n",
    "  \"\"\" Function to retrieve formatted cluster summary for analysis\"\"\"\n",
    "  cluster_summary_df_list = []\n",
    "  final_df_column_list = ['cluster', 'words', 'termWeights']\n",
    "  \n",
    "  vocab = lda_model.stages[0].vocabulary \n",
    "  topics = lda_model.stages[1].describeTopics()\n",
    "  topics_words = topics.rdd.map(lambda row: row['termIndices']) .map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "  \n",
    "  for idx, topic in enumerate(topics_words):\n",
    "    cluster_summary_dict = OrderedDict()\n",
    "    cluster_summary_dict['cluster'] = idx + 1\n",
    "    cluster_summary_dict['words'] = [word for word in topic]\n",
    "    cluster_summary_df_list.append(cluster_summary_dict)\n",
    "\n",
    "  summary_pre_df = spark.createDataFrame(cluster_summary_df_list)\n",
    "  summary_df = summary_pre_df.join(topics, summary_pre_df.cluster == topics.topic)[final_df_column_list]\n",
    "  #new_summary_df = expanded_frame(summary_df)[final_df_column_list]\n",
    "  return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_optimal_topics(topic_range, maxIteration, input_df):\n",
    "  \"\"\" Function to compute optimal topics within a custom input range \"\"\"\n",
    "  ll_vals, lp_vals, cluster_no = [], [], []\n",
    "  cv = CountVectorizer(inputCol='tokenized_sentiments', outputCol='features')\n",
    "  lda = LDA ()\n",
    "  lda_pipeline = Pipeline(stages=[cv, lda])\n",
    "  \n",
    "  for topic_no in topic_range:\n",
    "    paramap = {lda.k: topic_no, lda.maxIter:maxIteration, cv.vocabSize:100000, cv.minDF:5.0}\n",
    "    lda_i_model = lda_pipeline.fit (input_df, paramap)\n",
    "    lda_i_results = lda_i_model.transform (input_df)\n",
    "    \n",
    "    ll_vals.append(lda_i_model.stages[1].logLikelihood(lda_i_results))\n",
    "    lp_vals.append(lda_i_model.stages[1].logPerplexity(lda_i_results))\n",
    "    cluster_no.append(topic_no)\n",
    "  \n",
    "  return ll_vals, lp_vals, cluster_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concat(type):\n",
    "  \"\"\" Function to concat multiple Array columns in a dataframe into a single column\"\"\"\n",
    "  def concat_(*args):\n",
    "      return list(chain(*args))\n",
    "  return udf(concat_, ArrayType(type))\n",
    "\n",
    "#PySpark UDF\n",
    "concat_string_arrays = concat(StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_top_ngrams(input_df, cap):\n",
    "  \"\"\" Function to return top ngrams grouped by cluster\"\"\"\n",
    "  top_ngrams_perCluster = {}\n",
    "  \n",
    "  for itr in range(0, 12, 1):\n",
    "    input_df_sub = input_df.filter(\"cluster_predictions == %d\" % itr)\n",
    "    \n",
    "    voc_ngram = list(chain(*input_df_sub[['tokenized_sentiments']].toPandas()['tokenized_sentiments'].tolist()))\n",
    "    voc_ngram_freq = {key:len(list(group)) for key, group in groupby(voc_ngram)}\n",
    "    voc_ngram_freq_sorted = OrderedDict(sorted(voc_ngram_freq.items(), key=lambda it: it[1], reverse=True))\n",
    "    top_ngrams_i =  ' '.join(list(set([key.encode('utf-8') for key,value in voc_ngram_freq_sorted.items()[:3000]]))[:cap])\n",
    "    \n",
    "    top_ngrams_perCluster[itr] = top_ngrams_i\n",
    "  \n",
    "  return top_ngrams_perCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "twitter_sentiments = spark.read.csv('/mnt/lda/twitter_sentiment.csv', header=True) # Only taking a limited set\n",
    "twitter_sentiments = twitter_sentiments[['SentimentText']]\n",
    "twitter_sentiments = twitter_sentiments.withColumn(\"tokenized_sentiments\", preprocessing_udf(\"SentimentText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cluster</th><th>words</th><th>termWeights</th></tr></thead><tbody><tr><td>1</td><td>List(thank, wish, could, like, got, girl, , love, feel, know)</td><td>List(0.019832085774291935, 0.017667615192240544, 0.01278841902211623, 0.011497980713773105, 0.008808296122910276, 0.007938521356095735, 0.007317337164365658, 0.007141093835725717, 0.0063817789091509735, 0.005941255764071033)</td></tr><tr><td>2</td><td>List(said, thought, yeah, thing, exactly, mean, hate, myweakness, follow, second)</td><td>List(0.03355007465199728, 0.012330520052984035, 0.012009115449253394, 0.010138482418648, 0.010026986684539243, 0.009850389161256641, 0.009019471426372879, 0.00884479560446169, 0.00802454907682287, 0.007851805946270206)</td></tr><tr><td>3</td><td>List(love, wanna, happy, &amp;, u, get, great, lol, day, still)</td><td>List(0.01694661951468619, 0.014224388424454037, 0.012726501889125318, 0.012544694003537237, 0.012131699648805765, 0.01113067164982209, 0.010920503419947622, 0.010556856875636546, 0.008662035576269598, 0.008464833324140148)</td></tr><tr><td>4</td><td>List(know, go, cannot, new, want, like, much, get, that's, lol)</td><td>List(0.017885742334392003, 0.016150452459641116, 0.016025094114560663, 0.01256018472245425, 0.009629492534559745, 0.009618481037404585, 0.009583313167791324, 0.009235953388504622, 0.007909604283676142, 0.007491956161727438)</td></tr><tr><td>5</td><td>List(, good, i am, u, like, get, lol, time, thanks, day)</td><td>List(0.019199325260617323, 0.01502195282466297, 0.009995819493855867, 0.007161779038940221, 0.006584564544502578, 0.0053619964978811555, 0.005168854911937015, 0.005105750354268841, 0.004981154877219053, 0.004862563332871768)</td></tr><tr><td>6</td><td>List(oh, haha, suck, check, two, okay, love, next, talking, away)</td><td>List(0.03831686454509927, 0.01060752592295178, 0.010346307644073011, 0.009513023896561132, 0.009340676232261627, 0.009318685573472941, 0.00887796287217453, 0.0065372890494664005, 0.005621639767919185, 0.005521682373443904)</td></tr><tr><td>7</td><td>List(i am, think, come, sorry, working, u, phone, true, , da)</td><td>List(0.015580323108866735, 0.015370044738176623, 0.01534565738529194, 0.012133787055996487, 0.012111761866006493, 0.011917484139487605, 0.011821265005306648, 0.011501689157690505, 0.008620704450890233, 0.008460183144512958)</td></tr><tr><td>8</td><td>List(everyone, hey, follower, get, pay, add, using, day, train, link)</td><td>List(0.02875325552308516, 0.027086625903748704, 0.022273227169057587, 0.0162124964050247, 0.008924499094948751, 0.00805277279848437, 0.007297107843811206, 0.007245761787022959, 0.005829878821669545, 0.004821426518368476)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "twitter_sentiments_cv = CountVectorizer(inputCol='tokenized_sentiments', outputCol='features') # Creating featues column\n",
    "twitter_sentiments_lda = LDA () # Training a LDA Model\n",
    "Lda_Pipeline = Pipeline(stages=[twitter_sentiments_cv, twitter_sentiments_lda])\n",
    "\n",
    "# Parameterized values\n",
    "paramap = {twitter_sentiments_lda.k: 9, twitter_sentiments_lda.maxIter:50, twitter_sentiments_cv.vocabSize:100000, twitter_sentiments_cv.minDF:5.0}\n",
    "\n",
    "LdaModel = Lda_Pipeline.fit(twitter_sentiments, paramap)\n",
    "LdaResults = LdaModel.transform(twitter_sentiments)\n",
    "\n",
    "cluster_summary_df = return_cluster_summary(LdaModel)\n",
    "display (cluster_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n",
       "<span class=\"ansigreen\">&lt;command-2717300798909995&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n",
       "<span class=\"ansigreen\">      2</span> wordcloud <span class=\"ansiyellow\">=</span> WordCloud<span class=\"ansiyellow\">(</span>background_color<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;white&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      3</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span>t1<span class=\"ansiyellow\">.</span>imshow<span class=\"ansiyellow\">(</span>wordcloud<span class=\"ansiyellow\">.</span>generate<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos; &apos;</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>cluster_summary_df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;cluster == 1&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;words&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>encode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;utf-8&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      5</span> t2<span class=\"ansiyellow\">.</span>imshow<span class=\"ansiyellow\">(</span>wordcloud<span class=\"ansiyellow\">.</span>generate<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos; &apos;</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>cluster_summary_df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;cluster == 2&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;words&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>encode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;utf-8&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      6</span> t3<span class=\"ansiyellow\">.</span>imshow<span class=\"ansiyellow\">(</span>wordcloud<span class=\"ansiyellow\">.</span>generate<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos; &apos;</span><span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>cluster_summary_df<span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;cluster == 3&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;words&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">.</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>encode<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;utf-8&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/wordcloud/wordcloud.py</span> in <span class=\"ansicyan\">generate</span><span class=\"ansiblue\">(self, text)</span>\n",
       "<span class=\"ansigreen\">    603</span>         self<span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    604</span>         &quot;&quot;&quot;\n",
       "<span class=\"ansigreen\">--&gt; 605</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>generate_from_text<span class=\"ansiyellow\">(</span>text<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    606</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    607</span>     <span class=\"ansigreen\">def</span> _check_generated<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/wordcloud/wordcloud.py</span> in <span class=\"ansicyan\">generate_from_text</span><span class=\"ansiblue\">(self, text)</span>\n",
       "<span class=\"ansigreen\">    584</span>         self<span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    585</span>         &quot;&quot;&quot;\n",
       "<span class=\"ansigreen\">--&gt; 586</span><span class=\"ansiyellow\">         </span>words <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>process_text<span class=\"ansiyellow\">(</span>text<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    587</span>         self<span class=\"ansiyellow\">.</span>generate_from_frequencies<span class=\"ansiyellow\">(</span>words<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    588</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/wordcloud/wordcloud.py</span> in <span class=\"ansicyan\">process_text</span><span class=\"ansiblue\">(self, text)</span>\n",
       "<span class=\"ansigreen\">    551</span>         regexp <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>regexp <span class=\"ansigreen\">if</span> self<span class=\"ansiyellow\">.</span>regexp <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">None</span> <span class=\"ansigreen\">else</span> <span class=\"ansiblue\">r&quot;\\w[\\w&apos;]+&quot;</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    552</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">--&gt; 553</span><span class=\"ansiyellow\">         </span>words <span class=\"ansiyellow\">=</span> re<span class=\"ansiyellow\">.</span>findall<span class=\"ansiyellow\">(</span>regexp<span class=\"ansiyellow\">,</span> text<span class=\"ansiyellow\">,</span> flags<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    554</span>         <span class=\"ansired\"># remove stopwords</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    555</span>         words <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>word <span class=\"ansigreen\">for</span> word <span class=\"ansigreen\">in</span> words <span class=\"ansigreen\">if</span> word<span class=\"ansiyellow\">.</span>lower<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> stopwords<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/python/lib/python3.5/re.py</span> in <span class=\"ansicyan\">findall</span><span class=\"ansiblue\">(pattern, string, flags)</span>\n",
       "<span class=\"ansigreen\">    211</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    212</span>     Empty matches are included in the result.&quot;&quot;&quot;\n",
       "<span class=\"ansigreen\">--&gt; 213</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> _compile<span class=\"ansiyellow\">(</span>pattern<span class=\"ansiyellow\">,</span> flags<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>findall<span class=\"ansiyellow\">(</span>string<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    214</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    215</span> <span class=\"ansigreen\">def</span> finditer<span class=\"ansiyellow\">(</span>pattern<span class=\"ansiyellow\">,</span> string<span class=\"ansiyellow\">,</span> flags<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansired\">TypeError</span>: cannot use a string pattern on a bytes-like object</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8)) = plt.subplots(4, 2)\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "\n",
    "t1.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 1')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t2.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 2')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t3.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 3')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t4.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 4')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t5.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 5')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t6.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 6')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t7.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 7')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t8.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 8')[['words']].collect()[0].words).encode('utf-8')))\n",
    "\n",
    "t1.set_title('Cluster 1')\n",
    "t2.set_title('Cluster 2')\n",
    "t3.set_title('Cluster 3')\n",
    "t4.set_title('Cluster 4')\n",
    "t5.set_title('Cluster 5')\n",
    "t6.set_title('Cluster 6')\n",
    "t7.set_title('Cluster 7')\n",
    "t8.set_title('Cluster 8')\n",
    "\n",
    "t1.figure.set_size_inches(20, 20)\n",
    "t2.figure.set_size_inches(20, 20)\n",
    "t3.figure.set_size_inches(20, 20)\n",
    "t4.figure.set_size_inches(20, 20)\n",
    "t5.figure.set_size_inches(20, 20)\n",
    "t6.figure.set_size_inches(20, 20)\n",
    "t7.figure.set_size_inches(20, 20)\n",
    "t8.figure.set_size_inches(20, 20)\n",
    "\n",
    "t1.set_axis_off()\n",
    "t2.set_axis_off()\n",
    "t3.set_axis_off()\n",
    "t4.set_axis_off()\n",
    "t5.set_axis_off()\n",
    "t6.set_axis_off()\n",
    "t7.set_axis_off()\n",
    "t8.set_axis_off()\n",
    "display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_range = range(6, 15, 2)\n",
    "ll_vals, lp_vals, cluster_no = get_optimal_topics(topic_range, 50, twitter_sentiments.sample(False, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(cluster_no, ll_vals, xlabel='Clusters', ylabel='LogLikelihood Scores', title='Plot of LogLikelihood against number of clusters', color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(cluster_no, lp_vals, xlabel='Clusters', ylabel='LogPerplexity Scores', title='Plot of LogPerplexity against number of clusters', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterized values\n",
    "paramap = {twitter_sentiments_lda.k: 13, twitter_sentiments_lda.maxIter:50, twitter_sentiments_cv.vocabSize:100000, twitter_sentiments_cv.minDF:6.0}\n",
    "\n",
    "LdaModel = Lda_Pipeline.fit(twitter_sentiments, paramap)\n",
    "LdaResults_Best = LdaModel.transform(twitter_sentiments)\n",
    "\n",
    "cluster_summary_df_Best = return_cluster_summary(LdaModel)\n",
    "display (cluster_summary_df_Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8), (t9, t10), (t11, t12)) = plt.subplots(6, 2)\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "\n",
    "t1.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 1')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t2.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 2')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t3.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 3')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t4.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 4')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t5.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 5')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t6.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 6')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t7.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 7')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t8.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 8')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t9.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 9')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t10.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 10')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t11.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 11')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t12.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 12')[['words']].collect()[0].words).encode('utf-8')))\n",
    "\n",
    "t1.set_title('Cluster 1')\n",
    "t2.set_title('Cluster 2')\n",
    "t3.set_title('Cluster 3')\n",
    "t4.set_title('Cluster 4')\n",
    "t5.set_title('Cluster 5')\n",
    "t6.set_title('Cluster 6')\n",
    "t7.set_title('Cluster 7')\n",
    "t8.set_title('Cluster 8')\n",
    "t9.set_title('Cluster 9')\n",
    "t10.set_title('Cluster 10')\n",
    "t11.set_title('Cluster 11')\n",
    "t12.set_title('Cluster 12')\n",
    "\n",
    "t1.figure.set_size_inches(20, 20)\n",
    "t2.figure.set_size_inches(20, 20)\n",
    "t3.figure.set_size_inches(20, 20)\n",
    "t4.figure.set_size_inches(20, 20)\n",
    "t5.figure.set_size_inches(20, 20)\n",
    "t6.figure.set_size_inches(20, 20)\n",
    "t7.figure.set_size_inches(20, 20)\n",
    "t8.figure.set_size_inches(20, 20)\n",
    "t9.figure.set_size_inches(20, 20)\n",
    "t10.figure.set_size_inches(20, 20)\n",
    "t11.figure.set_size_inches(20, 20)\n",
    "t12.figure.set_size_inches(20, 20)\n",
    "\n",
    "t1.set_axis_off()\n",
    "t2.set_axis_off()\n",
    "t3.set_axis_off()\n",
    "t4.set_axis_off()\n",
    "t5.set_axis_off()\n",
    "t6.set_axis_off()\n",
    "t7.set_axis_off()\n",
    "t8.set_axis_off()\n",
    "t9.set_axis_off()\n",
    "t10.set_axis_off()\n",
    "t11.set_axis_off()\n",
    "t12.set_axis_off()\n",
    "display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = range(2, 5, 1)\n",
    "ngrams = [NGram(n=ngram, inputCol=\"tokenized_sentiments\", outputCol=\"{0}_grams\".format(ngram)) for ngram in ngram_range]\n",
    "ngrams_pipeline = Pipeline(stages=ngrams)\n",
    "ngram_df = ngrams_pipeline.fit(LdaResults).transform(LdaResults)\n",
    "\n",
    "# Merge Ngrams into a single column\n",
    "ngram_df = ngram_df.select(col('SentimentText'), col('tokenized_sentiments'), col('features'), col('topicDistribution'), concat_string_arrays(col(\"2_grams\"), col(\"3_grams\"), col(\"4_grams\")).alias('ngrams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_ngram = list(chain(*ngram_df[['ngrams']].toPandas()['ngrams'].tolist()))\n",
    "voc_ngram_freq = {key:len(list(group)) for key, group in groupby(voc_ngram)}\n",
    "voc_ngram_freq_sorted = OrderedDict(sorted(voc_ngram_freq.items(), key=lambda it: it[1], reverse=True))\n",
    "top_30_ngrams =  ' '.join(list(set([key.encode('utf-8') for key,value in voc_ngram_freq_sorted.items()[:3000]]))[:30])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "wordcloud = WordCloud(background_color='black')\n",
    "ax.imshow(wordcloud.generate(top_30_ngrams))\n",
    "ax.set_title('Top 30 NGrams')\n",
    "ax.figure.set_size_inches(10, 10)\n",
    "ax.set_axis_off()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "kmeans_inst = BisectingKMeans(featuresCol='features', predictionCol='cluster_predictions', maxIter=50, k=12, minDivisibleClusterSize=5.0, seed=42)\n",
    "kmeans_mod = kmeans_inst.fit(LdaResults)\n",
    "LdaResults_kmeans = kmeans_mod.transform(LdaResults)\n",
    "cluster_centers = kmeans_mod.clusterCenters()\n",
    "\n",
    "display (LdaResults_kmeans, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "clusters = []\n",
    "\n",
    "for cluster_i in range(6, 15, 2):\n",
    "    kmeans_i = BisectingKMeans(featuresCol='features', predictionCol='cluster_predictions', maxIter=30, k=cluster_i, minDivisibleClusterSize=5.0, seed=43)\n",
    "    model_i = kmeans_i.fit(LdaResults_kmeans.sample(False, 0.2, seed=42))\n",
    "    \n",
    "    # Updating the list\n",
    "    costs.append(model_i.computeCost(LdaResults_kmeans))\n",
    "    clusters.append(cluster_i)\n",
    "\n",
    "# plotter(x_vals, y_vals, xlabel, ylabel, title, color=None)\n",
    "plotter (clusters, costs, 'Clusters', 'Cost', 'Detecting Optimal Number of clusters using KMeans', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_ngrams_perCluster = get_top_ngrams(LdaResults_kmeans, 30)\n",
    "fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8), (t9, t10), (t11, t12)) = plt.subplots(6, 2)\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "\n",
    "t1.imshow(wordcloud.generate(top30_ngrams_perCluster.get(0)))\n",
    "t2.imshow(wordcloud.generate(top30_ngrams_perCluster.get(1)))\n",
    "t3.imshow(wordcloud.generate(top30_ngrams_perCluster.get(2)))\n",
    "t4.imshow(wordcloud.generate(top30_ngrams_perCluster.get(3)))\n",
    "t5.imshow(wordcloud.generate(top30_ngrams_perCluster.get(4)))\n",
    "t6.imshow(wordcloud.generate(top30_ngrams_perCluster.get(5)))\n",
    "t7.imshow(wordcloud.generate(top30_ngrams_perCluster.get(6)))\n",
    "t8.imshow(wordcloud.generate(top30_ngrams_perCluster.get(7)))\n",
    "t9.imshow(wordcloud.generate(top30_ngrams_perCluster.get(8)))\n",
    "t10.imshow(wordcloud.generate(top30_ngrams_perCluster.get(9)))\n",
    "t11.imshow(wordcloud.generate(top30_ngrams_perCluster.get(10)))\n",
    "t12.imshow(wordcloud.generate(top30_ngrams_perCluster.get(11)))\n",
    "\n",
    "t1.set_title('Cluster 1')\n",
    "t2.set_title('Cluster 2')\n",
    "t3.set_title('Cluster 3')\n",
    "t4.set_title('Cluster 4')\n",
    "t5.set_title('Cluster 5')\n",
    "t6.set_title('Cluster 6')\n",
    "t7.set_title('Cluster 7')\n",
    "t8.set_title('Cluster 8')\n",
    "t9.set_title('Cluster 9')\n",
    "t10.set_title('Cluster 10')\n",
    "t11.set_title('Cluster 11')\n",
    "t12.set_title('Cluster 12')\n",
    "\n",
    "t1.figure.set_size_inches(20, 20)\n",
    "t2.figure.set_size_inches(20, 20)\n",
    "t3.figure.set_size_inches(20, 20)\n",
    "t4.figure.set_size_inches(20, 20)\n",
    "t5.figure.set_size_inches(20, 20)\n",
    "t6.figure.set_size_inches(20, 20)\n",
    "t7.figure.set_size_inches(20, 20)\n",
    "t8.figure.set_size_inches(20, 20)\n",
    "t9.figure.set_size_inches(20, 20)\n",
    "t10.figure.set_size_inches(20, 20)\n",
    "t11.figure.set_size_inches(20, 20)\n",
    "t12.figure.set_size_inches(20, 20)\n",
    "\n",
    "t1.set_axis_off()\n",
    "t2.set_axis_off()\n",
    "t3.set_axis_off()\n",
    "t4.set_axis_off()\n",
    "t5.set_axis_off()\n",
    "t6.set_axis_off()\n",
    "t7.set_axis_off()\n",
    "t8.set_axis_off()\n",
    "t9.set_axis_off()\n",
    "t10.set_axis_off()\n",
    "t11.set_axis_off()\n",
    "t12.set_axis_off()\n",
    "display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LdaResults_kmeans.createOrReplaceTempView('LdaResults_kmeans')\n",
    "LdaResults_kmeans_summary = spark.sql('''with tbl1 as (select cluster_predictions AS summ_cluster_predictions, count(*) count_total from LdaResults_kmeans group by cluster_predictions) \n",
    "                             select tbl1.*, row_number() over (order by count_total desc) as cluster_prediction from tbl1 order by count_total desc''')\n",
    "LDA_Results_kmeans_summary_expanded = LdaResults_kmeans_summary.join(LdaResults_kmeans, LdaResults_kmeans_summary.summ_cluster_predictions == LdaResults_kmeans.cluster_predictions, how='inner')[['SentimentText', 'tokenized_sentiments', 'count_total', 'cluster_predictions', 'topicDistribution']]\n",
    "display (LDA_Results_kmeans_summary_expanded)\n",
    "spark.catalog.dropTempView('LdaResults_kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (LDA_Results_kmeans_summary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used in this notebook came from a specific user's Twitter feed, so, isn't a very great one to start with. However, both, LDA and Bisecting KMeans implementation demonstrated that 12 topics is an optimal number for topic modeling. \n",
    "The published notebook is available at - \n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3173713035751393/2689111109152757/2308983777460038/latest.html\n",
    "\n",
    "Note: Data Source used is those of the web scrapped data publicly available from Twitter API from personal twitter handle, and other twitter handles owned by me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "name": "TopicModeling_LDA_KMeans",
  "notebookId": 2689111109152757
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
