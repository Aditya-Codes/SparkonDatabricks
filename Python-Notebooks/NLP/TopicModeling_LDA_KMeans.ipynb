{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adityakumar.94/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adityakumar.94/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Computational and Visualisation Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import string, re\n",
    "from collections import OrderedDict\n",
    "from wordcloud import WordCloud\n",
    "from itertools import chain, groupby\n",
    "\n",
    "# Pyspark Packages\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer, NGram\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# NLTK Packages\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper, Utility and Wrapper Modules to assist in the later part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(token):\n",
    "  \"\"\" Function to return lemmatized token\"\"\"\n",
    "  return lemmatizer.lemmatize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation) \n",
    "default_stopwords = stopwords.words('english')\n",
    "custom_stopwords = ['charliecuntskies ', 'bts', 'imisscath ', 'imisscath', 'mcflyforgermany', 'charliecuntskies', 'delongeday', 'rlly', 'barakatday', 'blah']\n",
    "stopwords_f = set(default_stopwords + custom_stopwords)\n",
    "pattern_repl_list = [\".\", \",\", \"(\", \")\", \"!\", \"@\", \"#\"]\n",
    "\n",
    "replacement_patterns = [\n",
    "(r\"won't\", \"will not\"),\n",
    "(r\"haven't\", \"have not\"),\n",
    "(r\"can't\", \"cannot\"),\n",
    "(r\"i'm\", \"i am\"),\n",
    "(r\"ain't\", \"am not\"),\n",
    "(r\"(\\w+)'ll\", \"$1 will\"),\n",
    "(r\"(\\w+)n't\", \"$1 not\"),\n",
    "(r\"(\\w+)'ve\", \"$1 have\"),\n",
    "(r\"(\\w+)'s\", \"$1 is\"),\n",
    "(r\"(\\w+)'re\", \"$1 are\"),\n",
    "(r\"(\\w+)'d\", \"$1 would\"),\n",
    "]\n",
    "\n",
    "def preprocessed_chats (text):\n",
    "  \"\"\" Function to extract processed column over Spark Dataframe \"\"\"\n",
    "  text_token = text.lower().split()\n",
    "  \n",
    "  text_token_filtered = []\n",
    "  for token in text_token:\n",
    "    for pattern in pattern_repl_list: token = token.replace(pattern, '') # Limit patterns from pattern_repl_list\n",
    "    for pattern in replacement_patterns: token = token.replace(pattern[0], pattern[1]) # Limit patterns from replacement_patterns list\n",
    "    text_token_filtered.append(token)\n",
    "  \n",
    "  text_token_filtered_pre = [word for word in text_token_filtered if len(word) > 4]\n",
    "  text_sw_removed = [word for word in text_token_filtered if word not in stopwords_f]\n",
    "  text_punc_removed = [word for word in text_sw_removed if word not in punctuation]\n",
    "  text_lemmatized = [lemmatize(word) for word in text_punc_removed]\n",
    "  return text_lemmatized\n",
    "\n",
    "preprocessing_udf = udf(preprocessed_chats, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotter(x_vals, y_vals, xlabel, ylabel, title, color=None):\n",
    "  \"\"\" Custom Pyplot Plotter\"\"\"\n",
    "  if color is None:\n",
    "    color='red'\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x_vals, y_vals, color=color)\n",
    "  ax.set_title(title)\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel(ylabel)\n",
    "  display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expanded_frame(summary_df):\n",
    "  \"\"\" Function to return words and termWeights each for a particular cluster\"\"\"\n",
    "  df_list = []\n",
    "  for index, row in summary_df.toPandas().iterrows():\n",
    "    words_list = row['words']\n",
    "    words_weight = row['termWeights']\n",
    "\n",
    "    for itr in range(len(words_list)):\n",
    "      expanded_dict = OrderedDict()\n",
    "      expanded_dict['cluster'] = row['cluster']\n",
    "      expanded_dict['words'] = words_list[itr]\n",
    "      expanded_dict['termWeights'] = words_weight[itr]\n",
    "      df_list.append(expanded_dict)\n",
    "\n",
    "  new_df = spark.createDataFrame(df_list)\n",
    "  return (new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_cluster_summary(lda_model):\n",
    "  \"\"\" Function to retrieve formatted cluster summary for analysis\"\"\"\n",
    "  cluster_summary_df_list = []\n",
    "  final_df_column_list = ['cluster', 'words', 'termWeights']\n",
    "  \n",
    "  vocab = lda_model.stages[0].vocabulary \n",
    "  topics = lda_model.stages[1].describeTopics()\n",
    "  topics_words = topics.rdd.map(lambda row: row['termIndices']) .map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "  \n",
    "  for idx, topic in enumerate(topics_words):\n",
    "    cluster_summary_dict = OrderedDict()\n",
    "    cluster_summary_dict['cluster'] = idx + 1\n",
    "    cluster_summary_dict['words'] = [word for word in topic]\n",
    "    cluster_summary_df_list.append(cluster_summary_dict)\n",
    "\n",
    "  summary_pre_df = spark.createDataFrame(cluster_summary_df_list)\n",
    "  summary_df = summary_pre_df.join(topics, summary_pre_df.cluster == topics.topic)[final_df_column_list]\n",
    "  #new_summary_df = expanded_frame(summary_df)[final_df_column_list]\n",
    "  return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimal_topics(topic_range, maxIteration, input_df):\n",
    "  \"\"\" Function to compute optimal topics within a custom input range \"\"\"\n",
    "  ll_vals, lp_vals, cluster_no = [], [], []\n",
    "  cv = CountVectorizer(inputCol='tokenized_sentiments', outputCol='features')\n",
    "  lda = LDA ()\n",
    "  lda_pipeline = Pipeline(stages=[cv, lda])\n",
    "  \n",
    "  for topic_no in topic_range:\n",
    "    paramap = {lda.k: topic_no, lda.maxIter:maxIteration, cv.vocabSize:100000, cv.minDF:5.0}\n",
    "    lda_i_model = lda_pipeline.fit (input_df, paramap)\n",
    "    lda_i_results = lda_i_model.transform (input_df)\n",
    "    \n",
    "    ll_vals.append(lda_i_model.stages[1].logLikelihood(lda_i_results))\n",
    "    lp_vals.append(lda_i_model.stages[1].logPerplexity(lda_i_results))\n",
    "    cluster_no.append(topic_no)\n",
    "  \n",
    "  return ll_vals, lp_vals, cluster_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat(type):\n",
    "  \"\"\" Function to concat multiple Array columns in a dataframe into a single column\"\"\"\n",
    "  def concat_(*args):\n",
    "      return list(chain(*args))\n",
    "  return udf(concat_, ArrayType(type))\n",
    "\n",
    "#PySpark UDF\n",
    "concat_string_arrays = concat(StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_ngrams(input_df, cap):\n",
    "  \"\"\" Function to return top ngrams grouped by cluster\"\"\"\n",
    "  top_ngrams_perCluster = {}\n",
    "  \n",
    "  for itr in range(0, 12, 1):\n",
    "    input_df_sub = input_df.filter(\"cluster_predictions == %d\" % itr)\n",
    "    \n",
    "    voc_ngram = list(chain(*input_df_sub[['tokenized_sentiments']].toPandas()['tokenized_sentiments'].tolist()))\n",
    "    voc_ngram_freq = {key:len(list(group)) for key, group in groupby(voc_ngram)}\n",
    "    voc_ngram_freq_sorted = OrderedDict(sorted(voc_ngram_freq.items(), key=lambda it: it[1], reverse=True))\n",
    "    top_ngrams_i =  ' '.join(list(set([key.encode('utf-8') for key,value in voc_ngram_freq_sorted.items()[:3000]]))[:cap])\n",
    "    \n",
    "    top_ngrams_perCluster[itr] = top_ngrams_i\n",
    "  \n",
    "  return top_ngrams_perCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_sentiments = spark.read.csv('/mnt/lda/twitter_sentiment.csv', header=True) # Only taking a limited set\n",
    "twitter_sentiments = twitter_sentiments[['SentimentText']]\n",
    "twitter_sentiments = twitter_sentiments.withColumn(\"tokenized_sentiments\", preprocessing_udf(\"SentimentText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_sentiments_cv = CountVectorizer(inputCol='tokenized_sentiments', outputCol='features') # Creating featues column\n",
    "twitter_sentiments_lda = LDA () # Training a LDA Model\n",
    "Lda_Pipeline = Pipeline(stages=[twitter_sentiments_cv, twitter_sentiments_lda])\n",
    "\n",
    "# Parameterized values\n",
    "paramap = {twitter_sentiments_lda.k: 9, twitter_sentiments_lda.maxIter:50, twitter_sentiments_cv.vocabSize:100000, twitter_sentiments_cv.minDF:5.0}\n",
    "\n",
    "LdaModel = Lda_Pipeline.fit(twitter_sentiments, paramap)\n",
    "LdaResults = LdaModel.transform(twitter_sentiments)\n",
    "\n",
    "cluster_summary_df = return_cluster_summary(LdaModel)\n",
    "display (cluster_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8)) = plt.subplots(4, 2)\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "\n",
    "t1.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 1')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t2.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 2')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t3.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 3')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t4.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 4')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t5.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 5')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t6.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 6')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t7.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 7')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t8.imshow(wordcloud.generate(' '.join(cluster_summary_df.filter('cluster == 8')[['words']].collect()[0].words).encode('utf-8')))\n",
    "\n",
    "t1.set_title('Cluster 1')\n",
    "t2.set_title('Cluster 2')\n",
    "t3.set_title('Cluster 3')\n",
    "t4.set_title('Cluster 4')\n",
    "t5.set_title('Cluster 5')\n",
    "t6.set_title('Cluster 6')\n",
    "t7.set_title('Cluster 7')\n",
    "t8.set_title('Cluster 8')\n",
    "\n",
    "t1.figure.set_size_inches(20, 20)\n",
    "t2.figure.set_size_inches(20, 20)\n",
    "t3.figure.set_size_inches(20, 20)\n",
    "t4.figure.set_size_inches(20, 20)\n",
    "t5.figure.set_size_inches(20, 20)\n",
    "t6.figure.set_size_inches(20, 20)\n",
    "t7.figure.set_size_inches(20, 20)\n",
    "t8.figure.set_size_inches(20, 20)\n",
    "\n",
    "t1.set_axis_off()\n",
    "t2.set_axis_off()\n",
    "t3.set_axis_off()\n",
    "t4.set_axis_off()\n",
    "t5.set_axis_off()\n",
    "t6.set_axis_off()\n",
    "t7.set_axis_off()\n",
    "t8.set_axis_off()\n",
    "display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_range = range(6, 15, 2)\n",
    "ll_vals, lp_vals, cluster_no = get_optimal_topics(topic_range, 50, twitter_sentiments.sample(False, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter(cluster_no, ll_vals, xlabel='Clusters', ylabel='LogLikelihood Scores', title='Plot of LogLikelihood against number of clusters', color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter(cluster_no, lp_vals, xlabel='Clusters', ylabel='LogPerplexity Scores', title='Plot of LogPerplexity against number of clusters', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameterized values\n",
    "paramap = {twitter_sentiments_lda.k: 13, twitter_sentiments_lda.maxIter:50, twitter_sentiments_cv.vocabSize:100000, twitter_sentiments_cv.minDF:6.0}\n",
    "\n",
    "LdaModel = Lda_Pipeline.fit(twitter_sentiments, paramap)\n",
    "LdaResults_Best = LdaModel.transform(twitter_sentiments)\n",
    "\n",
    "cluster_summary_df_Best = return_cluster_summary(LdaModel)\n",
    "display (cluster_summary_df_Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8), (t9, t10), (t11, t12)) = plt.subplots(6, 2)\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "\n",
    "t1.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 1')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t2.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 2')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t3.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 3')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t4.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 4')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t5.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 5')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t6.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 6')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t7.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 7')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t8.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 8')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t9.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 9')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t10.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 10')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t11.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 11')[['words']].collect()[0].words).encode('utf-8')))\n",
    "t12.imshow(wordcloud.generate(' '.join(cluster_summary_df_Best.filter('cluster == 12')[['words']].collect()[0].words).encode('utf-8')))\n",
    "\n",
    "t1.set_title('Cluster 1')\n",
    "t2.set_title('Cluster 2')\n",
    "t3.set_title('Cluster 3')\n",
    "t4.set_title('Cluster 4')\n",
    "t5.set_title('Cluster 5')\n",
    "t6.set_title('Cluster 6')\n",
    "t7.set_title('Cluster 7')\n",
    "t8.set_title('Cluster 8')\n",
    "t9.set_title('Cluster 9')\n",
    "t10.set_title('Cluster 10')\n",
    "t11.set_title('Cluster 11')\n",
    "t12.set_title('Cluster 12')\n",
    "\n",
    "t1.figure.set_size_inches(20, 20)\n",
    "t2.figure.set_size_inches(20, 20)\n",
    "t3.figure.set_size_inches(20, 20)\n",
    "t4.figure.set_size_inches(20, 20)\n",
    "t5.figure.set_size_inches(20, 20)\n",
    "t6.figure.set_size_inches(20, 20)\n",
    "t7.figure.set_size_inches(20, 20)\n",
    "t8.figure.set_size_inches(20, 20)\n",
    "t9.figure.set_size_inches(20, 20)\n",
    "t10.figure.set_size_inches(20, 20)\n",
    "t11.figure.set_size_inches(20, 20)\n",
    "t12.figure.set_size_inches(20, 20)\n",
    "\n",
    "t1.set_axis_off()\n",
    "t2.set_axis_off()\n",
    "t3.set_axis_off()\n",
    "t4.set_axis_off()\n",
    "t5.set_axis_off()\n",
    "t6.set_axis_off()\n",
    "t7.set_axis_off()\n",
    "t8.set_axis_off()\n",
    "t9.set_axis_off()\n",
    "t10.set_axis_off()\n",
    "t11.set_axis_off()\n",
    "t12.set_axis_off()\n",
    "display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_range = range(2, 5, 1)\n",
    "ngrams = [NGram(n=ngram, inputCol=\"tokenized_sentiments\", outputCol=\"{0}_grams\".format(ngram)) for ngram in ngram_range]\n",
    "ngrams_pipeline = Pipeline(stages=ngrams)\n",
    "ngram_df = ngrams_pipeline.fit(LdaResults).transform(LdaResults)\n",
    "\n",
    "# Merge Ngrams into a single column\n",
    "ngram_df = ngram_df.select(col('SentimentText'), col('tokenized_sentiments'), col('features'), col('topicDistribution'), concat_string_arrays(col(\"2_grams\"), col(\"3_grams\"), col(\"4_grams\")).alias('ngrams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_ngram = list(chain(*ngram_df[['ngrams']].toPandas()['ngrams'].tolist()))\n",
    "voc_ngram_freq = {key:len(list(group)) for key, group in groupby(voc_ngram)}\n",
    "voc_ngram_freq_sorted = OrderedDict(sorted(voc_ngram_freq.items(), key=lambda it: it[1], reverse=True))\n",
    "top_30_ngrams =  ' '.join(list(set([key.encode('utf-8') for key,value in voc_ngram_freq_sorted.items()[:3000]]))[:30])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "wordcloud = WordCloud(background_color='black')\n",
    "ax.imshow(wordcloud.generate(top_30_ngrams))\n",
    "ax.set_title('Top 30 NGrams')\n",
    "ax.figure.set_size_inches(10, 10)\n",
    "ax.set_axis_off()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "kmeans_inst = BisectingKMeans(featuresCol='features', predictionCol='cluster_predictions', maxIter=50, k=12, minDivisibleClusterSize=5.0, seed=42)\n",
    "kmeans_mod = kmeans_inst.fit(LdaResults)\n",
    "LdaResults_kmeans = kmeans_mod.transform(LdaResults)\n",
    "cluster_centers = kmeans_mod.clusterCenters()\n",
    "\n",
    "display (LdaResults_kmeans, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costs = []\n",
    "clusters = []\n",
    "\n",
    "for cluster_i in range(6, 15, 2):\n",
    "    kmeans_i = BisectingKMeans(featuresCol='features', predictionCol='cluster_predictions', maxIter=30, k=cluster_i, minDivisibleClusterSize=5.0, seed=43)\n",
    "    model_i = kmeans_i.fit(LdaResults_kmeans.sample(False, 0.2, seed=42))\n",
    "    \n",
    "    # Updating the list\n",
    "    costs.append(model_i.computeCost(LdaResults_kmeans))\n",
    "    clusters.append(cluster_i)\n",
    "\n",
    "# plotter(x_vals, y_vals, xlabel, ylabel, title, color=None)\n",
    "plotter (clusters, costs, 'Clusters', 'Cost', 'Detecting Optimal Number of clusters using KMeans', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top30_ngrams_perCluster = get_top_ngrams(LdaResults_kmeans, 30)\n",
    "fig, ((t1,t2), (t3,t4), (t5,t6), (t7,t8), (t9, t10), (t11, t12)) = plt.subplots(6, 2)\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "\n",
    "t1.imshow(wordcloud.generate(top30_ngrams_perCluster.get(0)))\n",
    "t2.imshow(wordcloud.generate(top30_ngrams_perCluster.get(1)))\n",
    "t3.imshow(wordcloud.generate(top30_ngrams_perCluster.get(2)))\n",
    "t4.imshow(wordcloud.generate(top30_ngrams_perCluster.get(3)))\n",
    "t5.imshow(wordcloud.generate(top30_ngrams_perCluster.get(4)))\n",
    "t6.imshow(wordcloud.generate(top30_ngrams_perCluster.get(5)))\n",
    "t7.imshow(wordcloud.generate(top30_ngrams_perCluster.get(6)))\n",
    "t8.imshow(wordcloud.generate(top30_ngrams_perCluster.get(7)))\n",
    "t9.imshow(wordcloud.generate(top30_ngrams_perCluster.get(8)))\n",
    "t10.imshow(wordcloud.generate(top30_ngrams_perCluster.get(9)))\n",
    "t11.imshow(wordcloud.generate(top30_ngrams_perCluster.get(10)))\n",
    "t12.imshow(wordcloud.generate(top30_ngrams_perCluster.get(11)))\n",
    "\n",
    "t1.set_title('Cluster 1')\n",
    "t2.set_title('Cluster 2')\n",
    "t3.set_title('Cluster 3')\n",
    "t4.set_title('Cluster 4')\n",
    "t5.set_title('Cluster 5')\n",
    "t6.set_title('Cluster 6')\n",
    "t7.set_title('Cluster 7')\n",
    "t8.set_title('Cluster 8')\n",
    "t9.set_title('Cluster 9')\n",
    "t10.set_title('Cluster 10')\n",
    "t11.set_title('Cluster 11')\n",
    "t12.set_title('Cluster 12')\n",
    "\n",
    "t1.figure.set_size_inches(20, 20)\n",
    "t2.figure.set_size_inches(20, 20)\n",
    "t3.figure.set_size_inches(20, 20)\n",
    "t4.figure.set_size_inches(20, 20)\n",
    "t5.figure.set_size_inches(20, 20)\n",
    "t6.figure.set_size_inches(20, 20)\n",
    "t7.figure.set_size_inches(20, 20)\n",
    "t8.figure.set_size_inches(20, 20)\n",
    "t9.figure.set_size_inches(20, 20)\n",
    "t10.figure.set_size_inches(20, 20)\n",
    "t11.figure.set_size_inches(20, 20)\n",
    "t12.figure.set_size_inches(20, 20)\n",
    "\n",
    "t1.set_axis_off()\n",
    "t2.set_axis_off()\n",
    "t3.set_axis_off()\n",
    "t4.set_axis_off()\n",
    "t5.set_axis_off()\n",
    "t6.set_axis_off()\n",
    "t7.set_axis_off()\n",
    "t8.set_axis_off()\n",
    "t9.set_axis_off()\n",
    "t10.set_axis_off()\n",
    "t11.set_axis_off()\n",
    "t12.set_axis_off()\n",
    "display (fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LdaResults_kmeans.createOrReplaceTempView('LdaResults_kmeans')\n",
    "LdaResults_kmeans_summary = spark.sql('''with tbl1 as (select cluster_predictions AS summ_cluster_predictions, count(*) count_total from LdaResults_kmeans group by cluster_predictions) \n",
    "                             select tbl1.*, row_number() over (order by count_total desc) as cluster_prediction from tbl1 order by count_total desc''')\n",
    "LDA_Results_kmeans_summary_expanded = LdaResults_kmeans_summary.join(LdaResults_kmeans, LdaResults_kmeans_summary.summ_cluster_predictions == LdaResults_kmeans.cluster_predictions, how='inner')[['SentimentText', 'tokenized_sentiments', 'count_total', 'cluster_predictions', 'topicDistribution']]\n",
    "display (LDA_Results_kmeans_summary_expanded)\n",
    "spark.catalog.dropTempView('LdaResults_kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display (LDA_Results_kmeans_summary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used in this notebook came from a specific user's Facebook feed, so, isn't a very great one to start with. However, both, LDA and Bisecting KMeans implementation demonstrated that 12 topics is an optimal number for topic modeling. \n",
    "The published notebook is available at - \n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3173713035751393/2689111109152757/2308983777460038/latest.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "TopicModeling_LDA_KMeans",
  "notebookId": 2689111109152757
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
